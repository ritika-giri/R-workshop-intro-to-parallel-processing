---
output:
  html_document:
    df_print: paged
    code_download: TRUE
    toc: true
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

# Parallel Processing in R

## Introductory Concepts

### Serial vs Parallel processing

Parallel processing is the **simultaneous** execution of different pieces of a larger computation across multiple processors or cores. In theory, the idea is that if you can execute a computation in $X$ seconds on a single processor, then you should be able to execute it in $X/n$ seconds on $n$ processors. In practice, one must also consider overhead time for partitioning tasks between cores.

Consider that we have a series of functions to run, `f1`, `f2`, etc.

**Serial** processing means that `f1` runs first, and until `f1` completes, nothing else can run. Once `f1` completes, `f2` begins, and the process repeats.

**Parallel** processing (in the extreme) means that all the `f#` processes start simultaneously and run to completion on their own.

**Hidden parallelism**: You might have already used parallel processing without realizing it! Several R packages such as `caret` , `bigrf` and `GAMBoost` explicitly use parallel computing or [multithreading](https://www.wikiwand.com/en/Multithreading_(computer_architecture)) to speed up their execution and make it memory-efficient.

### What is a core?

Before we get started, let's look at some hardware terms:

-   **Thread**: The smallest sequence of programmed instructions that can be managed independently.

```{=html}
<!-- -->
```
-   **Core**: A physical unit that handles computation.

-   **Processor (or Socket)**: The silicon chip. There can be multiple cores on one processor.

-   **Node**: A single motherboard, with multiple processors. Usually one computer is referred to as a node.

-   **Cluster**: A group of interconnected nodes (or computers).

    As an example, [Quest](https://www.it.northwestern.edu/departments/it-services-support/research/computing/quest/) is a cluster of 1009 compute nodes with a total of 44,800 cores.

    ![](Hardware.png){width="324"}

### When should you use parallel processing?

One turns to parallel processing to solve one of three problems:

-   My program is **too slow**. Perhaps using more cores will make things run faster.

-   My problem is **too big**. Perhaps splitting the problem into multiple cores will give it access to enough memory to run effectively.

-   There are **too many** computations. Perhaps running them in parallel will save on time and memory requirements for my task.

### Concurrency : Multiple Independent Computations

For more cores to help, there has to be something for them to do. Find largely **independent** computations to occupy them. A good use-case would be running several sets of simulations with different starting seeds, or running bootstrap calculations for a test-statistic.

### Packages in R

-   `parallel` is a core R package which means it comes with your R installation. It merges two other packages that were used in the past (before 2011): `multicore` and `snow`.

-   `foreach` and `doParallel`

    -   `foreach` helps with executing loops in parallel, among other things.

    -   `doParallel` is a "parallel backend" for the *foreach* package. The user must register a parallel backend to use, otherwise *foreach* will execute tasks sequentially. Therefore, the *doParallel* package provides the mechanism needed to execute *foreach* loops in parallel.

```{r}
install.packages(c("foreach", "doParallel"))
```

## Using the `parallel` package

First, let's detect the number of cores in your machine!

```{r}
library(parallel)

numCores <- detectCores() # get the number of cores available
numCores
```

```{r}
print(detectCores())

# NOTE: Windows users may experience problems with using `mc*` family functions. Therefore, although your Windows machine may contain more than 1 core, we will set the number of cores to be 1 if you are working in Windows.

numCores <- if (Sys.info()["sysname"] == "Windows") 1 else detectCores()
```

### `mclapply()`

The simplest application of the `parallel` package is via the `mclapply()` function, which conceptually splits what might be a call to `lapply()` across multiple cores.

First, let's recap the `lapply()` function:

```{r}
?lapply
```

`lapply` returns a list of the same length as `X`, each element of which is the result of applying `FUN` to the corresponding element of `X` . For example:

```{r}
my_func <- function(x){
  x + 10 # add 10 to every number
}

lapply(1:10, my_func)
```

What is the type of object that's being returned?

```{r}
unlist(lapply(1:10, my_func))
```

The first two arguments to `mclapply()` are exactly the same as they are for `lapply()`. However, `mclapply()` has further arguments (that must be named), the most important of which is the `mc.cores` argument which you can use to specify the number of processors/cores you want to split the computation across.

NOTE: The `mc*` functions are generally not available to users of the Windows operating system because of how `parallel`manages the logistics of partitioning tasks across cores.

### Example: Bootstrapping a test-statistic

**Task**: I want to calculate some aggregate measures for my data such as the mean or the variance. I also want the 95% confidence intervals for these estimates to be more scientific.

**Problem**: However, confidence intervals cannot be analytically or mathematically calculated with a formula for all problems. In such cases, we simulate the data by "resampling" from the data at hand hundreds of thousands if not millions of times! We calculate the test-statistic for each of these simulated samples, and then find the 2.5% and 97.5% percentile values to estimate the 95% CI. This can be very time-consuming!

**Solution**: Parallel processing to the rescue! Each resample is independent of the others, therefore this problem can be solved much more efficiently in parallel.

**Demonstration:** We will be using the `iris` dataset that is pre-loaded in R. We will calculate the median of the column `Petal.Width` and use boot-strapping to generate 95% CIs for the estimated median.

```{r}
# Let's look at the dataset
View(iris)
names(iris)

# histogram of petal widths
hist(iris$Petal.Width)
```

We see that the histogram is skewed, so we decide to calculate the median (instead of the mean).

```{r}
data_median <- median(iris$Petal.Width) # direct estimate from raw data
data_median
```

However, we would like confidence intervals around the median estimate. Therefore, we decide to bootstrap the estimate by resampling (with replacement) from the data several times.

```{r}
# create your function to bootstrap
my_boot_function <- function(x){
  x_sample <- sample(x, replace=TRUE)
  median(x_sample)
}
```

Here's how we would do it the non-parallel way with `lapply`:

```{r}
# create replicates of your data vector to feed to lapply
X = replicate(n=5000, list(iris$Petal.Width))

sample_medians <- lapply(X, my_boot_function) # call lapply
sample_medians <- unlist(sample_medians) # Collapse list into vector
```

A 95% confidence interval would then take the 2.5th and 97.5th percentiles of this distribution:

```{r}
quantile(sample_medians, c(0.025, 0.975))
```

Now let's do this the parallel-way by wrapping it inside `mclapply`:

```{r}
sample_medians <- mclapply(X, my_boot_function, 
                           mc.cores = detectCores()) 

sample_medians <- unlist(sample_medians)  # Collapse list into vector
quantile(sample_medians, c(0.025, 0.975)) # get 95% CI
```

Let's see how much time each process takes using `system.time`:

```{r}
X = replicate(n=5000, list(iris$Petal.Width))

cat("The estimated time using lapply() function:")
system.time(lapply(X, my_boot_function))

cat("The estimated time using mclapply() function:")
system.time(mclapply(X, my_boot_function, mc.cores = numCores))
```

**NOTE:** Users running the code chunk above in Windows may not see a performance improvement since `numCores` is set to 1- but don't worry, you will experience the power of parallelism with the `foreach` package!

### EXERCISE 1

Change the number of replicate runs to 50,000 and see the difference in time elapsed.

```{r}
# create your replicate list
X <- 

t_reg <- system.time(lapply(X, my_boot_function))[3] # select elapsed time only
t_par <- system.time(mclapply(X, my_boot_function, mc.cores = numCores))[3]

t_reg / t_par # calculate fold-efficiency
```

### EXERCISE 2

Change the number of replicate runs to 100 and calculate the fold-efficiency. What do you notice?

```{r}


```

### EXERCISE 3

Write code to calculate the 95% confidence intervals for the mean of `Sepal.Width` in the `iris` data set. Estimate the CI by bootstrapping 5000 times, and using multiple cores in parallel.

```{r}
# create your replicate list to feed to mclapply
X = replicate(n= , list( ))

# create your bootstrap function for sample mean
my_boot_mean <- function(){
  
}

# call mclapply


# unlist the resulting list into a vector


# calculate the quantiles for 95% CI


```

[Concept question for bootstrapping:]{.underline} What would happen if you directly used the `mean` function inside `mclapply` instead of creating the `my_boot_mean` function?

## Using `foreach` and `doParallel`

The normal `for` loop in R looks like this:

```{r}
for (i in 1:3) {
  print(sqrt(i))
}
```

The `foreach` method is similar, but uses the sequential `%do%` operator to indicate an expression to run. Note the difference in the returned data structure.

```{r}
library(foreach)

foreach(i=1:3) %do% {
  sqrt(i)
}
```

In addition, `foreach` supports a parallelizable operator `%dopar%` from the `doParallel` package. `⁠%do%⁠` evaluates the expression sequentially, while `⁠%dopar%⁠` evaluates it in parallel. The results of evaluating `ex` are returned as a list by default, but this can be modified by means of the `.combine` argument.

Let's see an example using `%dopar%`.

First load the required libraries.

```{r}
library(foreach)
library(doParallel)
```

**IMPORTANT:** In order to run parallel processes, we will first need to register the parallel backend with the `foreach` package. If we do not do this, processes will run sequentially even if you use the `%dopar%` operator!!

```{r}
registerDoParallel(detectCores()) # you can set a different number if you don't want to use all the cores available
```

Sometimes, you might see the `makeCluster` command being used. The `registerDoParallel` function creates a cluster by automatically calling the `makeCluster` function.

To find out how many workers `foreach` is going to use, you can use the `getDoParWorkers` function.This is a useful sanity check that you're actually running in parallel.

```{r}
getDoParWorkers()

# other useful functions
getDoParName()
getDoParVersion()
```

Now run your loop in parallel using the `%dopar%` operator:

```{r}
foreach(i=1:3) %dopar% {
  sqrt(i)
}
```

To simplify output, `foreach` has the `.combine` parameter that can simplify return values.

```{r}
# Return a vector
foreach (i=1:3, .combine=c) %dopar% {
  sqrt(i)
}

# Return a data frame
foreach (i=1:3, .combine=rbind) %dopar% {
  sqrt(i)
}
```

### Example: Bootstrapping a test-statistic

Let's use the `iris` data set to run a bootstrap analysis.

This time, we want to get the slope and intercept for a logistic regression model which predicts the species as either "virginica" or "versicolor" based on the input parameter `Sepal.Length`.

```{r}
# remove setosa rows so only virginica and versicolor data remains
new_iris <- iris[iris$Species!="setosa",] 

# create a logistic regression model with Sepal.Length as input and species as output
result <- glm(new_iris$Species ~ new_iris$Sepal.Length, 
              family = binomial(logit))

result

# get the coefficients from the result object
coefficients(result)
```

Let's see how this would look if we boot-strapped this analysis to get several values for the coefficients of the logistic regression model.

```{r}
my_reg_boot_func <- function(df){
  df_sample <- dplyr::sample_n(df, 100, replace = TRUE) # sample 100 rows with replacement
  result <- glm(df_sample$Species ~ df_sample$Sepal.Length, family = binomial(logit))
  coefficients(result)
}
```

Now run a for loop to get several resampled values:

```{r}
trials <- 1000

# r <- foreach(i=1:trials, .combine = rbind) %do% {
#   my_reg_boot_func(new_iris)
# }
```

Let's wrap the for loop inside the timer function:

```{r}
system.time(
  r1 <- foreach(i=1:trials, .combine = rbind) %do% {
  my_reg_boot_func(new_iris)
}
)
```

What happens when we parallelize this process?

```{r}
# step 1 - register your cluster
registerDoParallel(detectCores())

# step 2 - run your parallel code
system.time(
  r2 <- foreach(i=1:trials, .combine = rbind) %dopar% {
  my_reg_boot_func(new_iris)
}
)

# step 3 - close the cluster
stopImplicitCluster()
```

NOTE: If you created a cluster object with `makeCluster` the you can use `stopCluster(cluster_name)` to close the corresponding cluster.

### EXERCISE 4

Convert the code below to run as a parallel process using `foreach`

```{r}
# sequential processing code
for (i in 1:100) {
  sqrt(i)
}

# your code using foreach below



```

### EXERCISE 5

What would you change in your answer above to return a vector of values instead of a list?

```{r}


```

**NOTE**: This is not a practical use of doParallel. `sqrt` executes far too quickly to be worth executing in parallel, even with a large number of iterations. With small tasks, the overhead of scheduling the task and returning the result can be greater than the time to execute the task itself, resulting in poor performance.

## Using the `BiocParallel` package

`BiocParallel` provides modified versions and novel implementation of functions for parallel evaluation, tailored to use with Bioconductor objects. While we will not cover it in this workshop, you can learn more about it [here](https://bioconductor.org/packages/devel/bioc/vignettes/BiocParallel/inst/doc/Introduction_To_BiocParallel.html).
