---
output:
  html_document:
    df_print: paged
    code_download: TRUE
    toc: true
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

# Parallel Processing in R

## Introductory Concepts

### Serial vs Parallel processing

Parallel processing is the **simultaneous** execution of different pieces of a larger computation across multiple processors or cores. The basic idea is that if you can execute a computation in $X$ seconds on a single processor, then you should be able to execute it in $X/n$ seconds on $n$ processors.

Consider that we have a series of functions to run, `f1`, `f2`, etc.

**Serial** processing means that `f1` runs first, and until `f1` completes, nothing else can run. Once `f1` completes, `f2` begins, and the process repeats.

**Parallel** processing (in the extreme) means that all the `f#` processes start simultaneously and run to completion on their own.

**Hidden parallelism**: You might have already used parallel processing without realizing it! Several R packages such as `caret` , `bigrf` and `GAMBoost` explicitly use parallel computing or [multithreading](https://www.wikiwand.com/en/Multithreading_(computer_architecture)) to speed up their execution and make it memory-efficient.

### What is a core?

Before we get started, let's look at some hardware terms:

-   **Core:** The unit of computation.

-   **Processor (or Socket)**: The silicon chip. There cab be multiple cores on one processor.

-   **Node**: A single motherboard, with multiple processors. Usually one computer is referred to as a node.

-   **Cluster**: A group of interconnected nodes (or computers).

### When should you use parallel processing?

One turns to parallel processing to solve one of three problems:

-   My program is **too slow**. Perhaps using more cores will make things run faster.

-   My problem is **too big**. Perhaps splitting the problem into multiple cores will give it access to enough memory to run effectively.

-   There are **too many** computations. Perhaps running them in parallel will save on time and memory requirements for my task.

### Concurrency : Multiple Independent Computations

For more cores to help, there has to be something for them to do. Find largely **independent** computations to occupy them. A good use-case would be running several sets of simulations with different starting seeds, or running bootstrap calculations for a test-statistic.

### Packages in R

-   `parallel` is a core R package which means it comes with your R installation. It merges two other packages that were used in the past (before 2011): `multicore` and `snow`.

-   `foreach` and `doParallel`

    -   `foreach` helps with executing loops in parallel, among other things.

    -   `doParallel` is a "parallel backend" for the *foreach* package. The user must register a parallel backend to use, otherwise *foreach* will execute tasks sequentially. Therefore, the *doParallel* package provides the mechanism needed to execute *foreach* loops in parallel.

## Using the `parallel` package

First, let's detect the number of cores in your machine!

```{r}
library(parallel)

numCores <- detectCores() # get the number of cores available
numCores
```

### `mclapply()`

The simplest application of the `parallel` package is via the `mclapply()` function, which conceptually splits what might be a call to `lapply()` across multiple cores.

First, let's recap the `lapply()` function:

```{r}
?lapply
```

`lapply` returns a list of the same length as `X`, each element of which is the result of applying `FUN` to the corresponding element of `X` . For example:

```{r}
my_func <- function(x){
  x + 10 # add 10 to every number
}

lapply(1:10, my_func)
```

What is the type of object that's being returned?

```{r}
unlist(lapply(1:10, my_func))
```

The first two arguments to `mclapply()` are exactly the same as they are for `lapply()`. However, `mclapply()` has further arguments (that must be named), the most important of which is the `mc.cores` argument which you can use to specify the number of processors/cores you want to split the computation across.

NOTE: The `mc*` functions are generally not available to users of the Windows operating system because of how `parallel`manages the logistics of partitioning tasks across cores.

### Example: Bootstrapping a test-statistic

Suppose I want to calculate some aggregate measures for my data. We will be using the `iris` dataset that is pre-loaded in R.

```{r}
View(iris)
names(iris)

# histogram of petal widths
hist(iris$Petal.Width)
```

We see that the histogram is skewed, so we decide to calculate the median.

```{r}
data_median <- median(iris$Petal.Width) # direct estimate from raw data
data_median
```

However, we would like confidence intervals around the median estimate. Therefore, we decide to bootstrap the estimate by resampling (with replacement) from the data several times.

```{r}
# create your function to bootstrap
my_boot_function <- function(x){
  x_sample <- sample(x, replace=TRUE)
  median(x_sample)
}
```

Here's how we would do it the non-parallel way with `lapply`:

```{r}
# create replicates of your data vector to feed to lapply
X = replicate(n=5000, list(iris$Petal.Width))

# call lapply
sample_medians <- lapply(X, my_boot_function)
sample_medians <- unlist(sample_medians) # Collapse list into vector
```

A 95% confidence interval would then take the 2.5th and 97.5th percentiles of this distribution:

```{r}
quantile(sample_medians, c(0.025, 0.975))
```

Now let's do this the parallel-way by wrapping it inside `mclapply`:

```{r}
sample_medians <- mclapply(X, my_boot_function, 
                           mc.cores = detectCores()) 

sample_medians <- unlist(sample_medians)  # Collapse list into vector
quantile(sample_medians, c(0.025, 0.975)) # get 95% CI
```

Let's see how much time each process takes (coded as "elapsed")

```{r}
X = replicate(n=5000, list(iris$Petal.Width))

cat("The estimated time using lapply() function:")
system.time(lapply(X, my_boot_function))

cat("The estimated time using mclapply() function:")
system.time(mclapply(X, my_boot_function, mc.cores = detectCores()))
```

### EXERCISE 1

Change the number of replicate runs to 50,000 and see the difference in time elapsed

```{r}
X <- 

t_reg <- system.time(lapply(X, my_boot_function))[3] # select elapsed time only
t_par <- system.time(mclapply(X, my_boot_function, mc.cores = detectCores()))[3]

t_reg / t_par # calculate fold-efficiency
```

### EXERCISE 2

Change the number of replicate runs to 100 and calculate the fold-efficiency. What do you notice?

```{r}


```

### EXERCISE 3

Write code to calculate the 95% confidence intervals for the mean of `Sepal.Width` in the `iris` data set. Estimate the CI by bootstrapping 5000 times, and using multiple cores in parallel.

```{r}
# create your list to feed to mclapply
X = replicate(n= , list( ))

# create your bootstrap function for sample mean
my_boot_mean <- function(){
  
}

# call to mclapply


# unlist the resulting list into a vector


# calculate the quantiles for 95% CI


```

[Concept question for bootstrapping:]{.underline} What would happen if you directly used the `mean` function inside `mclapply` instead of creating the `my_boot_mean` function?

## Using `foreach` and `doParallel`

The normal `for` loop in R looks like this:

```{r}
for (i in 1:3) {
  print(sqrt(i))
}
```

The `foreach` method is similar, but uses the sequential `%do%` operator to indicate an expression to run. Note the difference in the returned data structure.

```{r}
library(foreach)

foreach(i=1:3) %do% {
  sqrt(i)
}
```

In addition, `foreach` supports a parallelizable operator `%dopar%` from the `doParallel` package. `⁠%do%⁠` evaluates the expression sequentially, while `⁠%dopar%⁠` evaluates it in parallel. The results of evaluating `ex` are returned as a list by default, but this can be modified by means of the `.combine` argument.

Let's see an example using `%dopar%`.

First load the required libraries.

```{r}
library(foreach)
library(doParallel)
```

Next, register the parallel backend with the `foreach` package.

```{r}
registerDoParallel(detectCores()) # you can set a different number if you don't want to use all the cores available
```

Now run your loop in parallel:

```{r}
foreach(i=1:3) %dopar% {
  sqrt(i)
}
```

To simplify output, `foreach` has the `.combine` parameter that can simplify return values

```{r}
# Return a vector
foreach (i=1:3, .combine=c) %dopar% {
  sqrt(i)
}

# Return a data frame
foreach (i=1:3, .combine=rbind) %dopar% {
  sqrt(i)
}
```

### Example: Bootstrapping a test-statistic

Let's use the iris data set to run a bootstrap analysis.

This time, we want to get the slope and intercept for a logistic regression model which predicts the species as either "virginica" or "versicolor" based on the input parameter `Sepal.Length`.

```{r}
# remove setosa rows so only virginica and versicolor data remains
new_iris <- iris[iris$Species!="setosa",] 

# create a logistic regression model with Sepal.Length as input and species as output
result <- glm(new_iris$Species ~ new_iris$Sepal.Length, 
              family = binomial(logit))

result

# get the coefficients from the result object
coefficients(result)
```

Let's see how this would look if we boot-strapped this analysis to get several values for the coefficients of the logistic regression model.

```{r}
new_iris <- iris[iris$Species!="setosa",] 

my_reg_boot_func <- function(df){
  df_sample <- dplyr::sample_n(df, 100, replace = TRUE) # sample 100 rows with replacement
  result <- glm(df_sample$Species ~ df_sample$Sepal.Length, family = binomial(logit))
  coefficients(result)
}
```

Now run a for loop to get several resampled values:

```{r}
trials <- 1000

# r <- foreach(i=1:trials, .combine = rbind) %do% {
#   my_reg_boot_func(new_iris)
# }
```

Let's wrap the for loop inside the timer function:

```{r}
system.time(
  r1 <- foreach(i=1:trials, .combine = rbind) %do% {
  my_reg_boot_func(new_iris)
}
)
```

What happens when we parallelize this process?

```{r}
# step 1 - register your cluster
registerDoParallel(detectCores())

# step 2 - run your parallel code
system.time(
  r2 <- foreach(i=1:trials, .combine = rbind) %dopar% {
  my_reg_boot_func(new_iris)
}
)

# step 3 - close the cluster
stopImplicitCluster()
```

### EXERCISE 4

Convert the code below to run as a parallel process using `foreach`

```{r}
# sequential processing code
for (i in 1:100) {
  sqrt(i)
}

# your code using foreach below



```

### EXERCISE 5

What would you change in your answer above to return a vector of values instead of a list?

```{r}


```
